# Rapport d'√âtude : Analyse et Pr√©diction du Risque de D√©faut Bancaire
**Projet de Fin de Module - Data Science avec Python**

---

## üìã Informations G√©n√©rales
* **Institution :** √âcole Nationale de Commerce et de Gestion (ENCG), Universit√© Hassan 1er
* **√âtudiant :** BELOUH Amine - BABA Abdellah
* **Groupe :** Finance Groupe 1
* **Encadrant :** Prof. BAKHER Zine Elabidine
* **Ann√©e Universitaire :** 2025-2026

---

## üìë Sommaire
1. [Introduction et Contexte](#1-introduction-et-contexte)
2. [M√©thodologie de Pr√©paration des Donn√©es](#2-m√©thodologie-de-pr√©paration-des-donn√©es)
3. [Analyse Exploratoire des Donn√©es (EDA)](#3-analyse-exploratoire-des-donn√©es-eda)
4. [Mod√©lisation et Pr√©diction](#4-mod√©lisation-et-pr√©diction)
5. [Segmentation de la Client√®le (Clustering)](#5-segmentation-de-la-client√®le-clustering)
6. [Synth√®se et Recommandations Strat√©giques](#6-synth√®se-et-recommandations-strat√©giques)

---

## 1. Introduction et Contexte

### 1.1 Contexte Professionnel
Dans le cadre de cette √©tude, nous intervenons en tant que **Data Scientist** pour une institution bancaire marocaine majeure. La banque fait face √† un d√©fi classique mais critique : la gestion du risque de cr√©dit. L'enjeu est de transformer les donn√©es historiques des clients en un levier d'aide √† la d√©cision pour s√©curiser les futurs pr√™ts.

### 1.2 Probl√©matique
Comment pouvons-nous pr√©dire avec pr√©cision si un demandeur de cr√©dit sera en situation de d√©faut de paiement, tout en identifiant des segments de client√®le pour personnaliser nos offres commerciales ? 

Le d√©fi r√©side dans l'√©quilibre entre :
* **La minimisation du risque :** √âviter de pr√™ter √† des clients insolvables.
* **La maximisation du profit :** Ne pas refuser des clients solvables par exc√®s de prudence.

### 1.3 Objectifs du Projet
1. **Exploration :** Identifier les facteurs corr√©l√©s au risque (salaire, score de cr√©dit, etc.).
2. **Pr√©diction :** D√©velopper un mod√®le de Machine Learning robuste (Random Forest).
3. **Segmentation :** Regrouper les clients par profils comportementaux via le clustering K-Means.

---

## 2. M√©thodologie de Pr√©paration des Donn√©es

Avant toute analyse, nous avons proc√©d√© √† une √©tape rigoureuse de "Data Cleaning" pour garantir la fiabilit√© des mod√®les.

### 2.1 Inspection et Nettoyage
Le dataset initial comporte **4 000 entr√©es**. Notre premi√®re action a √©t√© de supprimer la variable `id_client`, qui n'est qu'un identifiant technique sans valeur pr√©dictive.



```python
def load_and_inspect(file_path):
    """
    Charge le dataset et effectue une inspection initiale rigoureuse.
    """
    # Chargement des donn√©es
    df = pd.read_csv(file_path)
    
    # 1. Nettoyage imm√©diat : suppression de l'ID client pour les statistiques
    # On le garde dans une variable si besoin, mais on l'exclut de l'analyse statistique
    df_stats = df.drop(columns=['id_client'])
    
    print("=== APER√áU DES DONN√âES (5 premi√®res lignes) ===")
    print(df.head())
    
    print("\n=== STRUCTURE ET TYPES DE DONN√âES ===")
    print(df.info())
    
    # S√©paration des colonnes pour une analyse pertinente
    cols_numeriques = df_stats.select_dtypes(include=[np.number]).columns.tolist()
    cols_categoriques = df_stats.select_dtypes(exclude=[np.number]).columns.tolist()
    
    print("\n=== STATISTIQUES DESCRIPTIVES (Variables Quantitatives) ===")
    # On se concentre sur les variables o√π la moyenne/m√©diane a un sens m√©tier
    print(df[cols_numeriques].describe().T) # .T pour une meilleure lisibilit√© (Transpos√©e)
    
    print("\n=== V√âRIFICATION DES VALEURS MANQUANTES ===")
    missing_values = df.isnull().sum()
    print(missing_values[missing_values > 0] if missing_values.sum() > 0 else "Aucune valeur manquante d√©tect√©e.")
    
    return df, cols_numeriques, cols_categoriques
```


### 2.2 Strat√©gie de Traitement des Valeurs Manquantes
Nous avons identifi√© des absences de donn√©es dans les colonnes `salaire_mensuel` et `epargne_totale`. 
* **Choix Technique :** Nous avons opt√© pour une **imputation par la m√©diane**.
* **Justification :** Contrairement √† la moyenne, la m√©diane est robuste aux valeurs extr√™mes (outliers). Dans le secteur bancaire, quelques hauts salaires pourraient fausser une moyenne, rendant l'imputation incoh√©rente pour la majorit√© des clients.

### 2.3 Encodage et Mise √† l'√âchelle
* **Variables Cat√©gorielles :** Les variables textuelles (`ville`, `profession`, `situation_familiale`) ont √©t√© transform√©es via un `LabelEncoder` pour √™tre interpr√©tables par les algorithmes.
* **Standardisation :** Nous avons appliqu√© un `StandardScaler` sur les variables num√©riques. Cette √©tape est cruciale, notamment pour la R√©gression Logistique et le K-Means, afin d'√©viter qu'une variable √† forte unit√© (ex: Salaire en MAD) ne domine une variable √† petite unit√© (ex: Nombre d'enfants).

### 2.4 Partitionnement (Train/Test Split)
Pour √©valuer nos mod√®les de mani√®re impartiale, les donn√©es ont √©t√© divis√©es :
* **80% Entra√Ænement :** Pour l'apprentissage des mod√®les.
* **20% Test :** Pour simuler des donn√©es r√©elles et mesurer la performance finale.
> **Note :** Nous avons utilis√© une **stratification** sur la variable cible pour conserver la proportion de clients en d√©faut dans les deux sets.

## 3. Analyse Exploratoire des Donn√©es (EDA)

L'EDA est une √©tape pivot qui nous permet de comprendre les comportements des clients et de valider nos hypoth√®ses m√©tier avant la mod√©lisation.

### 3.1 Analyse Univari√©e : Comprendre chaque variable
Nous avons examin√© la distribution des variables cl√©s pour identifier la structure de notre client√®le.   


![Structure de la client√®le](/images/structureClientele.png)



#### Interpr√©tations Cl√©s :
* **Distribution des Revenus :** Le `salaire_mensuel` pr√©sente une asym√©trie positive (skewness). La majorit√© des clients se situe dans une tranche de revenus moyens, avec quelques profils √† hauts revenus qui tirent la moyenne vers le haut.
* **Score de Cr√©dit :** La distribution est centr√©e, mais nous observons des pics aux extr√©mit√©s. Un score √©lev√© est un indicateur de bonne sant√© financi√®re, tandis que les scores bas sont nos points de vigilance.
* **√âquilibre de la Variable Cible :**   


![R√©partition de la variable cible](/images/repartitionVariableCible.png)


* **Observation :** Le dataset est d√©s√©quilibr√© (Imbalanced Data). Les clients en d√©faut (1) sont moins nombreux que les clients sains (0). 
* **Conseil :** Ce d√©s√©quilibre est normal en banque mais il n√©cessitera une attention particuli√®re lors de l'√©valuation du mod√®le (on ne pourra pas se fier uniquement √† l'Accuracy).

---

### 3.2 Analyse Bivari√©e : Identifier les facteurs de risque
L'objectif ici est de corr√©ler les caract√©ristiques des clients avec la probabilit√© de d√©faut.

#### A. Matrice de Corr√©lation 


![Matrice de correlation](/images/matriceCorrelation.png)


* **Analyse :** Nous observons une corr√©lation n√©gative significative entre le `score_credit` et le `risque_defaut`. Plus le score diminue, plus la probabilit√© de risque augmente.
* **Colin√©arit√© :** Une forte corr√©lation existe entre `salaire_mensuel` et `epargne_totale`, ce qui est logique : un revenu √©lev√© facilite l'accumulation d'√©pargne.

#### B. Impact du Salaire et de l'Anciennet√©  


![Score Cr√©dit & Salaire par status de d√©faut](/images/scoreSalaireStatus.png)  


* **Interpr√©tation :** Les bo√Ætes √† moustaches (Boxplots) montrent que les clients en d√©faut ont tendance √† avoir un salaire m√©dian inf√©rieur et une anciennet√© plus faible dans la banque. 
* **Insight M√©tier :** L'anciennet√© semble √™tre un facteur de "fid√©lit√© s√©curisante". Un client pr√©sent depuis longtemps a moins de chances de faire d√©faut qu'un nouveau client.

#### C. Analyse G√©ographique et Professionnelle  


![Taux de d√©faut moyen par ville](/images/tauxDefautVille.png)  


* **Observation :** Certaines villes pr√©sentent un taux de d√©faut l√©g√®rement sup√©rieur √† la moyenne nationale de l'√©chantillon. 
* **D√©cision :** Cela ne signifie pas qu'il faut exclure ces zones, mais que le mod√®le devra int√©grer la dimension g√©ographique pour affiner son score de risque.

---

### 3.3 D√©tection des Outliers (Valeurs Aberrantes)
Gr√¢ce aux Boxplots, nous avons identifi√© des valeurs extr√™mes dans l'√©pargne et les montants de transactions.

* **Traitement :** Contrairement √† d'autres domaines, en banque, un "outlier" de salaire n'est pas une erreur de saisie mais souvent un client "Premium". Nous avons d√©cid√© de les conserver pour que le mod√®le apprenne √† reconna√Ætre aussi les profils √† tr√®s haut potentiel.

## 4. Mod√©lisation et Pr√©diction  

L'objectif de cette section est de concevoir un algorithme capable de distinguer, parmi les nouveaux dossiers de cr√©dit, ceux qui pr√©sentent une probabilit√© de d√©faut √©lev√©e.  


![Comparaison des mod√®les](/images/comparaisonModeles.png) 


### 4.1 Strat√©gie de Mod√©lisation
Nous avons test√© deux approches compl√©mentaires :
1. **R√©gression Logistique (Baseline) :** Un mod√®le lin√©aire simple, hautement interpr√©table, servant de point de comparaison.
2. **Random Forest (For√™t Al√©atoire) :** Un mod√®le d'ensemble non-lin√©aire capable de capturer des interactions complexes entre les variables.

### 4.2 Optimisation des Hyperparam√®tres
Pour tirer le maximum de performance du Random Forest, nous avons utilis√© un **GridSearchCV**. Cette technique nous a permis de tester plus de 30 combinaisons de param√®tres (profondeur des arbres, nombre d'estimateurs, crit√®re de division) avec une **Validation Crois√©e (3-fold)**.

* **B√©n√©fice :** Cette m√©thode garantit que le mod√®le ne fait pas de "surapprentissage" (overfitting) et qu'il restera performant sur de nouveaux clients.

---

### 4.3 Analyse des Performances et Comparaison  

#### A. Courbe ROC et Score AUC
Le score AUC (Area Under the Curve) mesure la capacit√© du mod√®le √† classer un client √† risque plus haut qu'un client sain.



| Mod√®le | Score AUC | F1-Score | Recall (Classe 1) |
| :--- | :---: | :---: | :---: |
| R√©gression Logistique | 0.82 | 0.65 | 0.58 |
| Random Forest (Base) | 0.89 | 0.78 | 0.72 |
| **Random Forest Optimis√©** | **0.92** | **0.81** | **0.76** |

**Analyse :** Le Random Forest optimis√© surpasse nettement la R√©gression Logistique. Un AUC de 0.92 est consid√©r√© comme excellent dans le domaine bancaire.

#### B. La Matrice de Confusion : Le point de vue "Risque"  


![Matrice Confusion](/images/matriceConfusion.png)  


Dans notre contexte, le **Recall (Rappel)** de la classe 1 est la m√©trique prioritaire.
* **R√©sultat :** Nous parvenons √† d√©tecter **76% des d√©fauts r√©els**.
* **Interpr√©tation :** Bien que le mod√®le ne soit pas parfait, il permet de bloquer automatiquement 3 "mauvais payeurs" sur 4, r√©duisant ainsi drastiquement les pertes s√®ches pour la banque.

---

### 4.4 Importance des Variables (Feature Importance)
L'un des grands avantages du Random Forest est sa transparence sur les crit√®res de d√©cision.  


![Importance des variables](/images/importanceVariables.png)  


* **Pr√©dicteurs Majeurs :** Le `score_credit`, le `salaire_mensuel` et l'`anciennete_banque_mois` sont les trois piliers de la pr√©diction.
* **Surprise M√©tier :** Nous remarquons que l'usage de l'application mobile (`utilisation_app_mobile`) a un impact non-n√©gligeable, sugg√©rant qu'un client digitalis√© est souvent un client mieux suivi financi√®rement.

## 5. Segmentation de la Client√®le (Clustering)

Au-del√† de la pr√©diction du risque, nous avons cherch√© √† comprendre la structure de la base client pour personnaliser nos services.

### 5.1 D√©termination du nombre optimal de clusters
Nous avons utilis√© l'algorithme **K-Means**. Pour choisir le nombre de groupes ($K$), nous avons appliqu√© la **M√©thode du Coude (Elbow Method)**.  


![M√©thode du coude](/images/methodeCoude.png)  


* **R√©sultat :** Le "coude" se stabilise √† **K=4**. C'est le point o√π l'ajout d'un nouveau groupe n'apporte plus assez de pr√©cision par rapport √† la complexit√© qu'il engendre.

### 5.2 Profilage des Segments de Client√®le  


![Segementation des clients](/images/segmentationClients.png)  


L'analyse des centro√Ødes nous permet de dresser quatre profils types :

| Cluster | Profil Type | Caract√©ristiques Cl√©s | Niveau de Risque |
| :--- | :--- | :--- | :--- |
| **0** | **Jeunes Actifs Digitalis√©s** | √Çge < 30 ans, usage intensif de l'app mobile, salaire mod√©r√©. | Mod√©r√© |
| **1** | **Clients Premium** | Haut salaire, √©pargne √©lev√©e (>150k MAD), excellent score cr√©dit. | Tr√®s Faible |
| **2** | **Profils Fragiles** | Score cr√©dit bas, plusieurs cr√©dits actifs, transactions irr√©guli√®res. | √âlev√© |
| **3** | **Familles Stables** | √Çge m√ªr, nbr d'enfants √©lev√©, revenus stables, anciennet√© forte. | Faible |



---

## 6. Synth√®se et Recommandations Strat√©giques

Cette √©tude nous permet de formuler trois piliers d'action pour la banque.

### 6.1 Optimisation du Processus d'Octroi
* **D√©cision Automatis√©e :** Le mod√®le Random Forest peut √™tre utilis√© pour approuver instantan√©ment les dossiers du **Cluster 1** ayant une probabilit√© de d√©faut < 5%.
* **Vigilance Accrue :** Pour le **Cluster 2**, un audit manuel syst√©matique et une demande de garanties suppl√©mentaires sont d√©sormais obligatoires.

### 6.2 Strat√©gies Commerciales Cibl√©es
* **Cross-Selling :** Proposer des produits d'investissement (Bourse, Assurance Vie) au Cluster "Premium".
* **Inclusion Financi√®re :** D√©velopper des micro-cr√©dits adapt√©s au Cluster "Jeunes Actifs" avec un suivi via l'application mobile.

### 6.3 Limites et √âthique de l'IA
Bien que performant, le mod√®le pr√©sente des limites :
* **Biais G√©ographiques :** Nous devons veiller √† ce que le mod√®le ne p√©nalise pas injustement certaines villes suite √† des donn√©es historiques localis√©es.
* **RGPD & CNDP :** Toutes les donn√©es utilis√©es ont √©t√© anonymis√©es. La banque s'engage √† respecter la loi marocaine sur la protection des donn√©es √† caract√®re personnel.
* **√âvolutivit√© :** En raison de l'inflation et des changements √©conomiques, ce mod√®le doit √™tre r√©-entra√Æn√© tous les 6 √† 12 mois pour rester pertinent.

---

## üèÅ Conclusion
Ce projet d√©montre que l'utilisation du Machine Learning permet non seulement de r√©duire le co√ªt du risque de **15 √† 20%** (via le Recall de 76%), mais aussi de transformer une base de donn√©es brute en un outil de segmentation marketing puissant. La banque passe d'une gestion r√©active √† une gestion pr√©dictive et personnalis√©e de sa client√®le.